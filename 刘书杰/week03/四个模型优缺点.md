BERT 是基于 Transformer 架构的预训练语言模型，
所以代码是加载了预训练模型去微调。
优点：

强大的上下文理解能力：BERT 是双向的，能够同时捕捉到上下文中的信息，尤其在长文本中能处理长距离的依赖关系。

高精度：在很多 NLP 任务（如文本分类、命名实体识别、问答系统等）上，BERT 都表现出色，通常超越传统的机器学习方法。

多任务学习：BERT 模型可以通过微调应用到多种 NLP 任务，灵活性强。

预训练-微调框架：BERT 的预训练模型可以大规模共享，减少从头开始训练的时间和计算资源。

缺点：

计算资源消耗大：BERT 模型大，训练和推理时需要大量的计算资源，特别是在资源受限的情况下难以应用。需要gpu

推理速度慢：由于其复杂的架构，BERT 在实际推理过程中速度较慢，特别是在需要实时响应的应用中可能需要额外的优化。

TF-IDF + ML 
是一种传统的文本处理方法，适用于小数据集且不需要太复杂语义理解的任务，但对复杂任务表现有限。看见用sklean去实现的机器学习任务
优点：
简单高效：TF-IDF 和传统机器学习方法简单且计算开销较小。对于小规模的数据集，效果良好，且训练和推理速度较快。

不依赖大数据：与深度学习方法不同，TF-IDF + ML 不需要大量的训练数据，适用于数据量较小的情况。

解释性强：传统机器学习模型（如决策树、逻辑回归）通常具有较好的可解释性，可以追溯到具体的特征对预测结果的贡献。
缺点：

无法捕捉语义信息：TF-IDF 只能考虑单词出现频率，无法捕捉到单词之间的语义关系，也无法处理长距离依赖问题。

特征维度高：TF-IDF 转换后的特征矩阵通常维度很高，导致计算和存储的开销增大。

无法处理上下文：对于复杂的文本分类任务，TF-IDF 和传统机器学习模型往往无法像 BERT 那样理解上下文信息和长程依赖。

Prompting 
主要依赖于大型预训练语言模型，设计特定的提示来引导模型生成相关的输出，而不需要针对特定任务进行微调。
优点：

快速适应新任务：通过调整提示语句，无需重新训练模型，即可将预训练的语言模型应用到多种任务中。

无需大规模标注数据：由于不需要微调模型，可以通过简单的提示语实现对新任务的适应，尤其对于少样本任务非常有效。

灵活性强：可以灵活地调整任务、目标和生成的内容，只要设计合适的提示即可。

缺点：

提示设计难度：虽然模型本身预训练了大量知识，但如何设计一个有效的提示来引导模型进行正确的推理仍然是一个挑战。

依赖大模型：通常需要像 GPT-3 或 BERT 这样的大型预训练模型。对于中小型公司或资源有限的环境，可能不可行。

任务限定性：对于一些特定任务，prompting 可能无法完全替代专门的微调，需要任务的匹配性强才能取得较好效果。
