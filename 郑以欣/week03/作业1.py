import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import time
from sklearn.model_selection import train_test_split

# è¯»å–æ•°æ®
dataset = pd.read_csv("../Week03/dataset.csv", sep="\t", header=None)
texts = dataset[0].tolist()
string_labels = dataset[1].tolist()

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ7:3ï¼‰
train_texts, test_texts, train_labels, test_labels = train_test_split(
    texts, string_labels, test_size=0.3, random_state=42, stratify=string_labels
)

# åˆ›å»ºæ ‡ç­¾æ˜ å°„
label_to_index = {label: i for i, label in enumerate(set(string_labels))}
train_numerical_labels = [label_to_index[label] for label in train_labels]
test_numerical_labels = [label_to_index[label] for label in test_labels]

# åˆ›å»ºå­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„
char_to_index = {'<pad>': 0}
for text in texts:  # ä½¿ç”¨æ‰€æœ‰æ–‡æœ¬æ„å»ºè¯è¡¨
    for char in text:
        if char not in char_to_index:
            char_to_index[char] = len(char_to_index)

index_to_char = {i: char for char, i in char_to_index.items()}
vocab_size = len(char_to_index)
index_to_label = {i: label for label, i in label_to_index.items()}

max_len = 40


# è‡ªå®šä¹‰æ•°æ®é›†
class CharRNNDataset(Dataset):
    def __init__(self, texts, labels, char_to_index, max_len):
        self.texts = texts
        self.labels = torch.tensor(labels, dtype=torch.long)
        self.char_to_index = char_to_index
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        indices = [self.char_to_index.get(char, 0) for char in text[:self.max_len]]
        indices += [0] * (self.max_len - len(indices))
        return torch.tensor(indices, dtype=torch.long), self.labels[idx]


# åˆ›å»ºæ•°æ®é›†
train_dataset = CharRNNDataset(train_texts, train_numerical_labels, char_to_index, max_len)
test_dataset = CharRNNDataset(test_texts, test_numerical_labels, char_to_index, max_len)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


# å®šä¹‰ä¸åŒçš„æ¨¡å‹ç±»
class RNNClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, rnn_type='rnn'):
        super(RNNClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        if rnn_type.lower() == 'lstm':
            self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        elif rnn_type.lower() == 'gru':
            self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)
        else:  # é»˜è®¤ä½¿ç”¨ç®€å•RNN
            self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)

        self.fc = nn.Linear(hidden_dim, output_dim)
        self.rnn_type = rnn_type

    def forward(self, x):
        embedded = self.embedding(x)

        if self.rnn_type.lower() == 'lstm':
            rnn_out, (hidden, _) = self.rnn(embedded)
        else:  # RNN æˆ– GRU
            rnn_out, hidden = self.rnn(embedded)

        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšçŠ¶æ€
        out = self.fc(hidden.squeeze(0))
        return out


# è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
def train_and_evaluate(model_type, model_name, num_epochs=10, lr=0.001):
    print(f"\n{'=' * 60}")
    print(f"è®­ç»ƒ {model_name} æ¨¡å‹")
    print(f"{'=' * 60}")

    # åˆå§‹åŒ–æ¨¡å‹
    embedding_dim = 64
    hidden_dim = 128
    output_dim = len(label_to_index)

    model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, rnn_type=model_type)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # è®­ç»ƒæ¨¡å‹
    start_time = time.time()
    train_losses = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for batch_idx, (inputs, labels) in enumerate(train_loader):
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        epoch_loss = running_loss / len(train_loader)
        train_losses.append(epoch_loss)
        print(f"Epoch [{epoch + 1:2d}/{num_epochs}], è®­ç»ƒæŸå¤±: {epoch_loss:.4f}")

    training_time = time.time() - start_time

    # è¯„ä¼°æ¨¡å‹
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total

    print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%")

    return {
        'model_type': model_type,
        'model_name': model_name,
        'train_losses': train_losses,
        'accuracy': accuracy,
        'training_time': training_time,
        'model': model
    }


# åˆ†åˆ«è®­ç»ƒä¸‰ç§æ¨¡å‹
results = []

# 1. è®­ç»ƒRNNæ¨¡å‹
rnn_result = train_and_evaluate('rnn', 'Simple RNN', num_epochs=10)
results.append(rnn_result)

# 2. è®­ç»ƒLSTMæ¨¡å‹
lstm_result = train_and_evaluate('lstm', 'LSTM', num_epochs=10)
results.append(lstm_result)

# 3. è®­ç»ƒGRUæ¨¡å‹
gru_result = train_and_evaluate('gru', 'GRU', num_epochs=10)
results.append(gru_result)

# æ‰“å°å¯¹æ¯”ç»“æœ
print(f"\n{'=' * 80}")
print(f"{'æ¨¡å‹å¯¹æ¯”ç»“æœ':^80}")
print(f"{'=' * 80}")
print(f"{'æ¨¡å‹åç§°':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<15} {'è®­ç»ƒæ—¶é—´(ç§’)':<15} {'æœ€ç»ˆè®­ç»ƒæŸå¤±':<15}")
print(f"{'-' * 80}")

for result in results:
    print(f"{result['model_name']:<15} {result['accuracy']:<15.2f}% "
          f"{result['training_time']:<15.2f} {result['train_losses'][-1]:<15.4f}")

print(f"{'=' * 80}")

# æ‰¾å‡ºæœ€ä½³æ¨¡å‹
best_result = max(results, key=lambda x: x['accuracy'])
print(f"\nğŸ‰ æœ€ä½³æ¨¡å‹: {best_result['model_name']}")
print(f"   æµ‹è¯•å‡†ç¡®ç‡: {best_result['accuracy']:.2f}%")


# ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹ç¤ºä¾‹
def classify_text_gru(text, model, char_to_index, max_len, index_to_label):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    indices = [char_to_index.get(char, 0) for char in text[:max_len]]
    indices += [0] * (max_len - len(indices))
    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0)

    model.eval()
    with torch.no_grad():
        output = model(input_tensor)

    _, predicted_index = torch.max(output, 1)
    predicted_index = predicted_index.item()
    predicted_label = index_to_label[predicted_index]

    return predicted_label


# æµ‹è¯•ç¤ºä¾‹æ–‡æœ¬
print(f"\n{'=' * 80}")
print(f"{'é¢„æµ‹ç¤ºä¾‹':^80}")
print(f"{'=' * 80}")

test_cases = [
    "å¸®æˆ‘å¯¼èˆªåˆ°åŒ—äº¬",
    "æŸ¥è¯¢æ˜å¤©åŒ—äº¬çš„å¤©æ°”",
    "æ’­æ”¾å‘¨æ°ä¼¦çš„éŸ³ä¹",
    "ä»Šå¤©ä¸Šæµ·çš„æ¸©åº¦æ€ä¹ˆæ ·",
    "å¸¦æˆ‘å»æœ€è¿‘çš„åŠ æ²¹ç«™"
]

best_model = best_result['model']

for test_text in test_cases:
    try:
        predicted = classify_text_gru(test_text, best_model, char_to_index, max_len, index_to_label)
        print(f"è¾“å…¥: '{test_text}'")
        print(f"é¢„æµ‹: '{predicted}'\n")
    except:
        print(f"è¾“å…¥: '{test_text}'")
        print(f"é¢„æµ‹: 'æœªçŸ¥ç±»åˆ«' (å¯èƒ½ä¸åœ¨è®­ç»ƒæ ‡ç­¾ä¸­)\n")

# å¯è§†åŒ–è®­ç»ƒæŸå¤±å¯¹æ¯”
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
for result in results:
    plt.plot(result['train_losses'], label=result['model_name'], linewidth=2)

plt.xlabel('Epoch')
plt.ylabel('Training Loss')
plt.title('Training Loss Comparison: RNN vs LSTM vs GRU')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
save_path = './æäº¤çš„ä½œä¸š/week03/rnn_lstm_gru_comparison.png'
plt.savefig('rnn_lstm_gru_comparison.png', dpi=150, bbox_inches='tight')

print(f"\nğŸ“Š è®­ç»ƒæŸå¤±å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º 'rnn_lstm_gru_comparison.png'")
