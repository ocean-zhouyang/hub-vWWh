Model1:bert
核心作用:BERT 是Bidirectional Encoder Representations from Transformers的缩写，中文译为基于 Transformer 的双向编码器表征，是谷歌 2018 年提出的预训练语言模型，在本项目中用于汽车用户意图识别的高精度文本分类，能精准区分导航、空调调节等相似汽车意图，适配车载、客服、市场分析多场景的文本分类需求。

优点:

双向编码能捕捉上下文深层语义，语义理解能力强，对汽车行业易混淆意图的识别精度最高
预训练权重可直接调用，仅需微调分类任务层，开发效率高，支持批量部署适配多文本推理
泛化能力强，能适配汽车场景的口语化、书面化、碎片化等多样文本类型
拓展性强，新增汽车意图类别仅需微调分类头，无需重构模型架构
鲁棒性较好，对车载语音中的语气词、轻微错别字等文本噪声有一定容错性
缺点:

算力要求高，训练微调依赖 GPU，纯 CPU 环境下推理速度大幅下降
推理速度慢于正则、TF-IDF模型，车载端实时部署需额外做蒸馏、剪枝等轻量化处理
可解释性差，属于“黑箱模型”，需额外集成 SHAP/LIME 等工具满足业务溯源需求
小样本场景下微调易过拟合，新增小众汽车意图时需做数据增强，成本较高
模型更新需重新微调训练，相比规则模型维护成本中等
Model2:prompt
核心作用:基于Prompt提示学习的GPT大模型，是本项目中GPT大模型的实际应用方式，通过TF-IDF匹配汽车行业相似意图样本构造动态提示词，引导GPT大模型完成意图识别，核心适配小样本、复杂模糊的汽车意图识别场景。

优点:

对标注数据量要求低，仅需少量汽车意图样本即可达到高精度，小样本场景优势显著
无需设计复杂的任务层，仅通过提示词引导模型输出，GPT模型调用层代码逻辑简洁
继承GPT大模型的强语义理解能力，能处理汽车场景的复合意图、长文本舆情等复杂文本
适配汽车场景的文本多样性与灵活性，无需对文本做分词、去停用词等复杂预处理
拓展性极强，新增汽车意图仅需补充少量样本，无需重新训练模型
缺点:

提示词设计依赖行业经验，需结合汽车意图特征反复调试，动态样本匹配不当会直接影响精度
依赖网络与API服务（云端GPT）或高配置硬件（本地GPT），车载无网/弱网场景部署稳定性差
复杂复合意图的提示词构造难度高，多意图同时识别时效果不稳定
推理速度慢，且为逐样本推理，不支持汽车客服、舆情分析的高并发场景
使用成本高，云端调用按Token计费，批量处理汽车舆情会产生高额费用，本地部署算力成本极高
Model3:regex
核心作用:用正则表达式做关键词精准匹配，处理汽车场景中明确、结构化的指令式文本规则，是车载智能座舱核心指令识别的轻量模型。

优点:

完全可解释，匹配逻辑透明，适合汽车客服、车载系统等合规性、溯源性要求高的场景
无需数据标注和模型训练，直接编写汽车意图关键词规则即可上线，开发效率极高
推理速度最快，毫秒级处理，完美适配车载语音助手的实时响应需求
鲁棒性可控，对“打开空调”“导航到加油站”等车载核心指令的匹配精度100%
简单汽车意图新增/更新仅需补充规则，分钟级上线，迭代效率高
缺点:

泛化能力极差，仅能匹配固定关键词，对车载口语化、变体表述的文本完全失效
规则维护成本高，汽车意图表述方式多样，需持续编写大量规则，难以适配多场景
无法处理汽车场景的隐含语义和复合意图，如“导航到4S店并播放音乐”仅能识别单一意图
抗噪声能力弱，易受车载语音中的语气词、错别字、特殊符号等无关字符影响
识别覆盖率有天花板，无法通过规则覆盖汽车用户的所有意图表述方式
Model4:tfidf_ml
核心作用:TF-IDF提取文本词频特征，再结合LR/随机森林等分类器完成汽车意图识别，是本项目的基础基线模型，适配轻量部署的汽车客服、简单舆情分析场景。

优点:

实现简单，Sklearn、joblib等成熟库可直接调用，开发调试成本低
可解释性强，能输出汽车意图的核心关键词重要性，满足业务溯源需求
训练推理速度快，纯 CPU 即可运行，适配轻量部署的汽车客服后台
对车载语音、客服简短留言等短文本处理效果好，适配车载核心场景
模型训练与更新效率高，千级样本即可训练，新增意图补充少量数据即可重新训练
缺点:

特征表达能力弱，仅能捕捉词频统计特征，无法理解汽车意图的语义关联，相似意图易混淆
泛化能力有限，对汽车场景的新表述、低频次词的适应性差，误判率较高
需人工做大量特征工程，新增汽车文本类型时需重新设计分词、去停用词等规则
对汽车市场分析中的长文本舆情处理效果差，词频分散导致特征提取失效
对文本预处理依赖度高，分词错误、停用词过滤不当会直接影响识别精度
