{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e570c663-220c-447d-ba87-3a61cdb247b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d5725cac-4ced-4f56-ac97-6a9c85d5bdb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/home/jiahaoli/212/notebooks/selflearn/pytorch/Datasets/bert/weibo_senti_100k.csv\")  # 替换为你的训练数据集路径\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(dataset.iloc[:,0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "02300553-a1fd-4c82-a9ee-7d078cbcb9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>美~~~~~[爱你]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>梦想有多大，舞台就有多大![鼓掌]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119983</th>\n",
       "      <td>0</td>\n",
       "      <td>一公里不到，县医院那个天桥下右拐200米就到了！//@谢礼恒: 我靠。这个太霸道了！离224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119984</th>\n",
       "      <td>0</td>\n",
       "      <td>今天真冷啊，难道又要穿棉袄了[晕]？今年的春天真的是百变莫测啊[抓狂]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119985</th>\n",
       "      <td>0</td>\n",
       "      <td>最近几天就没停止过！！！[伤心]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119986</th>\n",
       "      <td>0</td>\n",
       "      <td>//@毒药女流氓:[怒] 很惨!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119987</th>\n",
       "      <td>0</td>\n",
       "      <td>呢??@杰?Kelena ？！[抓狂] ?搞乜鬼？？！！想知？入去GOtrip睇睇： htt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119988 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                             review\n",
       "0           1              ﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]\n",
       "1           1  @张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心...\n",
       "2           1  姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢/...\n",
       "3           1                                         美~~~~~[爱你]\n",
       "4           1                                  梦想有多大，舞台就有多大![鼓掌]\n",
       "...       ...                                                ...\n",
       "119983      0  一公里不到，县医院那个天桥下右拐200米就到了！//@谢礼恒: 我靠。这个太霸道了！离224...\n",
       "119984      0                今天真冷啊，难道又要穿棉袄了[晕]？今年的春天真的是百变莫测啊[抓狂]\n",
       "119985      0                                   最近几天就没停止过！！！[伤心]\n",
       "119986      0                                   //@毒药女流氓:[怒] 很惨!\n",
       "119987      0  呢??@杰?Kelena ？！[抓狂] ?搞乜鬼？？！！想知？入去GOtrip睇睇： htt...\n",
       "\n",
       "[119988 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f9e9ec99-e6c0-48d4-9247-2779b3ffabed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, train_label, test_label = train_test_split(\n",
    "    list(dataset.iloc[:,1].values),\n",
    "    lbl.transform(dataset.iloc[:,0].values),\n",
    "    test_size=0.2,\n",
    "    stratify=dataset.iloc[:,0].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ce041f30-c4de-4b6d-abf4-66286dc2b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiahaoli/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练的BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/jiahaoli/huggingface/models/bert')\n",
    "train_encoding = tokenizer(x_train, truncation=True, padding=True, max_length=64)\n",
    "test_encoding = tokenizer(x_test, truncation=True, padding=True, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "96ced5eb-5499-4488-bc1f-3e4637790856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    # 获取单个样本的方法\n",
    "    def __getitem__(self, idx):\n",
    "        # 从编码字典中提取input_ids, attention_mask等，并转换为PyTorch张量\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # 添加标签，并转换为张量\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "    # 返回数据集总样本数的方法\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# 实例化自定义数据集\n",
    "train_dataset = NewsDataset(train_encoding, train_label) # 单个样本读取的数据集\n",
    "test_dataset = NewsDataset(test_encoding, test_label)\n",
    "\n",
    "# 使用DataLoader创建批量数据加载器\n",
    "# batch_size=16：每个批次包含16个样本\n",
    "# shuffle=True：在每个epoch开始时打乱数据，以提高模型泛化能力\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) # 批量读取样本\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for batch_data in train_loader:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9cc220f1-5376-4110-b18a-495dbd79da66",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/jiahaoli/huggingface/models/bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Epoch: 0 ----------------\n",
      "epoth: 0, iter_num: 100, loss: 0.0649, 1.67%\n",
      "epoth: 0, iter_num: 200, loss: 0.2686, 3.33%\n",
      "epoth: 0, iter_num: 300, loss: 0.1483, 5.00%\n",
      "epoth: 0, iter_num: 400, loss: 0.0527, 6.67%\n",
      "epoth: 0, iter_num: 500, loss: 0.0276, 8.33%\n",
      "epoth: 0, iter_num: 600, loss: 0.6119, 10.00%\n",
      "epoth: 0, iter_num: 700, loss: 0.0186, 11.67%\n",
      "epoth: 0, iter_num: 800, loss: 0.1829, 13.33%\n",
      "epoth: 0, iter_num: 900, loss: 0.2538, 15.00%\n",
      "epoth: 0, iter_num: 1000, loss: 0.0367, 16.67%\n",
      "epoth: 0, iter_num: 1100, loss: 0.0339, 18.33%\n",
      "epoth: 0, iter_num: 1200, loss: 0.0397, 20.00%\n",
      "epoth: 0, iter_num: 1300, loss: 0.0620, 21.67%\n",
      "epoth: 0, iter_num: 1400, loss: 0.0230, 23.33%\n",
      "epoth: 0, iter_num: 1500, loss: 0.2743, 25.00%\n",
      "epoth: 0, iter_num: 1600, loss: 0.0691, 26.67%\n",
      "epoth: 0, iter_num: 1700, loss: 0.1044, 28.33%\n",
      "epoth: 0, iter_num: 1800, loss: 0.0630, 30.00%\n",
      "epoth: 0, iter_num: 1900, loss: 0.1042, 31.67%\n",
      "epoth: 0, iter_num: 2000, loss: 0.0325, 33.33%\n",
      "epoth: 0, iter_num: 2100, loss: 0.2511, 35.00%\n",
      "epoth: 0, iter_num: 2200, loss: 0.0247, 36.67%\n",
      "epoth: 0, iter_num: 2300, loss: 0.0290, 38.33%\n",
      "epoth: 0, iter_num: 2400, loss: 0.1580, 40.00%\n",
      "epoth: 0, iter_num: 2500, loss: 0.1575, 41.67%\n",
      "epoth: 0, iter_num: 2600, loss: 0.0656, 43.33%\n",
      "epoth: 0, iter_num: 2700, loss: 0.0039, 45.00%\n",
      "epoth: 0, iter_num: 2800, loss: 0.4178, 46.67%\n",
      "epoth: 0, iter_num: 2900, loss: 0.0377, 48.33%\n",
      "epoth: 0, iter_num: 3000, loss: 0.2196, 50.00%\n",
      "epoth: 0, iter_num: 3100, loss: 0.2815, 51.67%\n",
      "epoth: 0, iter_num: 3200, loss: 0.2749, 53.33%\n",
      "epoth: 0, iter_num: 3300, loss: 0.1682, 55.00%\n",
      "epoth: 0, iter_num: 3400, loss: 0.5137, 56.67%\n",
      "epoth: 0, iter_num: 3500, loss: 0.0299, 58.33%\n",
      "epoth: 0, iter_num: 3600, loss: 0.0365, 60.00%\n",
      "epoth: 0, iter_num: 3700, loss: 0.2552, 61.67%\n",
      "epoth: 0, iter_num: 3800, loss: 0.0348, 63.33%\n",
      "epoth: 0, iter_num: 3900, loss: 0.0392, 65.00%\n",
      "epoth: 0, iter_num: 4000, loss: 0.7698, 66.67%\n",
      "epoth: 0, iter_num: 4100, loss: 0.0735, 68.33%\n",
      "epoth: 0, iter_num: 4200, loss: 0.0099, 70.00%\n",
      "epoth: 0, iter_num: 4300, loss: 0.1180, 71.67%\n",
      "epoth: 0, iter_num: 4400, loss: 0.0764, 73.33%\n",
      "epoth: 0, iter_num: 4500, loss: 0.0233, 75.00%\n",
      "epoth: 0, iter_num: 4600, loss: 0.2980, 76.67%\n",
      "epoth: 0, iter_num: 4700, loss: 0.0200, 78.33%\n",
      "epoth: 0, iter_num: 4800, loss: 0.2276, 80.00%\n",
      "epoth: 0, iter_num: 4900, loss: 0.3056, 81.67%\n",
      "epoth: 0, iter_num: 5000, loss: 0.0532, 83.33%\n",
      "epoth: 0, iter_num: 5100, loss: 0.0556, 85.00%\n",
      "epoth: 0, iter_num: 5200, loss: 0.2134, 86.67%\n",
      "epoth: 0, iter_num: 5300, loss: 0.0410, 88.33%\n",
      "epoth: 0, iter_num: 5400, loss: 0.0158, 90.00%\n",
      "epoth: 0, iter_num: 5500, loss: 0.0860, 91.67%\n",
      "epoth: 0, iter_num: 5600, loss: 0.2822, 93.33%\n",
      "epoth: 0, iter_num: 5700, loss: 0.1486, 95.00%\n",
      "epoth: 0, iter_num: 5800, loss: 0.1624, 96.67%\n",
      "epoth: 0, iter_num: 5900, loss: 0.1684, 98.33%\n",
      "epoth: 0, iter_num: 6000, loss: 0.0218, 100.00%\n",
      "Epoch: 0, Average training loss: 0.1815\n",
      "Accuracy: 0.9384\n",
      "Average testing loss: 0.1607\n",
      "-------------------------------\n",
      "------------Epoch: 1 ----------------\n",
      "epoth: 1, iter_num: 100, loss: 0.1403, 1.67%\n",
      "epoth: 1, iter_num: 200, loss: 0.2423, 3.33%\n",
      "epoth: 1, iter_num: 300, loss: 0.0574, 5.00%\n",
      "epoth: 1, iter_num: 400, loss: 0.0400, 6.67%\n",
      "epoth: 1, iter_num: 500, loss: 0.1723, 8.33%\n",
      "epoth: 1, iter_num: 600, loss: 0.0148, 10.00%\n",
      "epoth: 1, iter_num: 700, loss: 0.0581, 11.67%\n",
      "epoth: 1, iter_num: 800, loss: 0.2082, 13.33%\n",
      "epoth: 1, iter_num: 900, loss: 0.0047, 15.00%\n",
      "epoth: 1, iter_num: 1000, loss: 0.0210, 16.67%\n",
      "epoth: 1, iter_num: 1100, loss: 0.1750, 18.33%\n",
      "epoth: 1, iter_num: 1200, loss: 0.1305, 20.00%\n",
      "epoth: 1, iter_num: 1300, loss: 0.1157, 21.67%\n",
      "epoth: 1, iter_num: 1400, loss: 0.2035, 23.33%\n",
      "epoth: 1, iter_num: 1500, loss: 0.2063, 25.00%\n",
      "epoth: 1, iter_num: 1600, loss: 0.0035, 26.67%\n",
      "epoth: 1, iter_num: 1700, loss: 0.0743, 28.33%\n",
      "epoth: 1, iter_num: 1800, loss: 0.0587, 30.00%\n",
      "epoth: 1, iter_num: 1900, loss: 0.1924, 31.67%\n",
      "epoth: 1, iter_num: 2000, loss: 0.2824, 33.33%\n",
      "epoth: 1, iter_num: 2100, loss: 0.0904, 35.00%\n",
      "epoth: 1, iter_num: 2200, loss: 0.4321, 36.67%\n",
      "epoth: 1, iter_num: 2300, loss: 0.0337, 38.33%\n",
      "epoth: 1, iter_num: 2400, loss: 0.5539, 40.00%\n",
      "epoth: 1, iter_num: 2500, loss: 0.1852, 41.67%\n",
      "epoth: 1, iter_num: 2600, loss: 0.1418, 43.33%\n",
      "epoth: 1, iter_num: 2700, loss: 0.0228, 45.00%\n",
      "epoth: 1, iter_num: 2800, loss: 0.0290, 46.67%\n",
      "epoth: 1, iter_num: 2900, loss: 0.0657, 48.33%\n",
      "epoth: 1, iter_num: 3000, loss: 0.3156, 50.00%\n",
      "epoth: 1, iter_num: 3100, loss: 0.1875, 51.67%\n",
      "epoth: 1, iter_num: 3200, loss: 0.0983, 53.33%\n",
      "epoth: 1, iter_num: 3300, loss: 0.5686, 55.00%\n",
      "epoth: 1, iter_num: 3400, loss: 0.0297, 56.67%\n",
      "epoth: 1, iter_num: 3500, loss: 0.2542, 58.33%\n",
      "epoth: 1, iter_num: 3600, loss: 0.4357, 60.00%\n",
      "epoth: 1, iter_num: 3700, loss: 0.2721, 61.67%\n",
      "epoth: 1, iter_num: 3800, loss: 0.0118, 63.33%\n",
      "epoth: 1, iter_num: 3900, loss: 0.0945, 65.00%\n",
      "epoth: 1, iter_num: 4000, loss: 0.2917, 66.67%\n",
      "epoth: 1, iter_num: 4100, loss: 0.0322, 68.33%\n",
      "epoth: 1, iter_num: 4200, loss: 0.1279, 70.00%\n",
      "epoth: 1, iter_num: 4300, loss: 0.0182, 71.67%\n",
      "epoth: 1, iter_num: 4400, loss: 0.1788, 73.33%\n",
      "epoth: 1, iter_num: 4500, loss: 0.4675, 75.00%\n",
      "epoth: 1, iter_num: 4600, loss: 0.0331, 76.67%\n",
      "epoth: 1, iter_num: 4700, loss: 0.1188, 78.33%\n",
      "epoth: 1, iter_num: 4800, loss: 0.0669, 80.00%\n",
      "epoth: 1, iter_num: 4900, loss: 0.1330, 81.67%\n",
      "epoth: 1, iter_num: 5000, loss: 0.0427, 83.33%\n",
      "epoth: 1, iter_num: 5100, loss: 0.4740, 85.00%\n",
      "epoth: 1, iter_num: 5200, loss: 0.5398, 86.67%\n",
      "epoth: 1, iter_num: 5300, loss: 0.1394, 88.33%\n",
      "epoth: 1, iter_num: 5400, loss: 0.1200, 90.00%\n",
      "epoth: 1, iter_num: 5500, loss: 0.2410, 91.67%\n",
      "epoth: 1, iter_num: 5600, loss: 0.0901, 93.33%\n",
      "epoth: 1, iter_num: 5700, loss: 0.0895, 95.00%\n",
      "epoth: 1, iter_num: 5800, loss: 0.0407, 96.67%\n",
      "epoth: 1, iter_num: 5900, loss: 0.5577, 98.33%\n",
      "epoth: 1, iter_num: 6000, loss: 0.3726, 100.00%\n",
      "Epoch: 1, Average training loss: 0.1604\n",
      "Accuracy: 0.9380\n",
      "Average testing loss: 0.1590\n",
      "-------------------------------\n",
      "------------Epoch: 2 ----------------\n",
      "epoth: 2, iter_num: 100, loss: 0.0721, 1.67%\n",
      "epoth: 2, iter_num: 200, loss: 0.0685, 3.33%\n",
      "epoth: 2, iter_num: 300, loss: 0.2155, 5.00%\n",
      "epoth: 2, iter_num: 400, loss: 0.3167, 6.67%\n",
      "epoth: 2, iter_num: 500, loss: 0.0167, 8.33%\n",
      "epoth: 2, iter_num: 600, loss: 0.0372, 10.00%\n",
      "epoth: 2, iter_num: 700, loss: 0.0314, 11.67%\n",
      "epoth: 2, iter_num: 800, loss: 0.3277, 13.33%\n",
      "epoth: 2, iter_num: 900, loss: 0.0148, 15.00%\n",
      "epoth: 2, iter_num: 1000, loss: 0.3050, 16.67%\n",
      "epoth: 2, iter_num: 1100, loss: 0.0286, 18.33%\n",
      "epoth: 2, iter_num: 1200, loss: 0.0239, 20.00%\n",
      "epoth: 2, iter_num: 1300, loss: 0.1272, 21.67%\n",
      "epoth: 2, iter_num: 1400, loss: 0.2712, 23.33%\n",
      "epoth: 2, iter_num: 1500, loss: 0.3882, 25.00%\n",
      "epoth: 2, iter_num: 1600, loss: 0.2144, 26.67%\n",
      "epoth: 2, iter_num: 1700, loss: 0.0019, 28.33%\n",
      "epoth: 2, iter_num: 1800, loss: 0.1065, 30.00%\n",
      "epoth: 2, iter_num: 1900, loss: 0.0462, 31.67%\n",
      "epoth: 2, iter_num: 2000, loss: 0.0520, 33.33%\n",
      "epoth: 2, iter_num: 2100, loss: 0.1951, 35.00%\n",
      "epoth: 2, iter_num: 2200, loss: 0.1440, 36.67%\n",
      "epoth: 2, iter_num: 2300, loss: 0.2005, 38.33%\n",
      "epoth: 2, iter_num: 2400, loss: 0.2129, 40.00%\n",
      "epoth: 2, iter_num: 2500, loss: 0.2217, 41.67%\n",
      "epoth: 2, iter_num: 2600, loss: 0.1390, 43.33%\n",
      "epoth: 2, iter_num: 2700, loss: 0.1714, 45.00%\n",
      "epoth: 2, iter_num: 2800, loss: 0.1638, 46.67%\n",
      "epoth: 2, iter_num: 2900, loss: 0.0508, 48.33%\n",
      "epoth: 2, iter_num: 3000, loss: 0.2325, 50.00%\n",
      "epoth: 2, iter_num: 3100, loss: 0.3234, 51.67%\n",
      "epoth: 2, iter_num: 3200, loss: 0.1923, 53.33%\n",
      "epoth: 2, iter_num: 3300, loss: 0.0173, 55.00%\n",
      "epoth: 2, iter_num: 3400, loss: 0.3839, 56.67%\n",
      "epoth: 2, iter_num: 3500, loss: 0.0034, 58.33%\n",
      "epoth: 2, iter_num: 3600, loss: 0.0063, 60.00%\n",
      "epoth: 2, iter_num: 3700, loss: 0.0326, 61.67%\n",
      "epoth: 2, iter_num: 3800, loss: 0.1923, 63.33%\n",
      "epoth: 2, iter_num: 3900, loss: 0.2136, 65.00%\n",
      "epoth: 2, iter_num: 4000, loss: 0.0079, 66.67%\n",
      "epoth: 2, iter_num: 4100, loss: 0.1125, 68.33%\n",
      "epoth: 2, iter_num: 4200, loss: 0.2047, 70.00%\n",
      "epoth: 2, iter_num: 4300, loss: 0.2780, 71.67%\n",
      "epoth: 2, iter_num: 4400, loss: 0.3694, 73.33%\n",
      "epoth: 2, iter_num: 4500, loss: 0.0247, 75.00%\n",
      "epoth: 2, iter_num: 4600, loss: 0.1119, 76.67%\n",
      "epoth: 2, iter_num: 4700, loss: 0.0144, 78.33%\n",
      "epoth: 2, iter_num: 4800, loss: 0.0998, 80.00%\n",
      "epoth: 2, iter_num: 4900, loss: 0.0433, 81.67%\n",
      "epoth: 2, iter_num: 5000, loss: 0.2992, 83.33%\n",
      "epoth: 2, iter_num: 5100, loss: 0.2980, 85.00%\n",
      "epoth: 2, iter_num: 5200, loss: 0.1244, 86.67%\n",
      "epoth: 2, iter_num: 5300, loss: 0.1158, 88.33%\n",
      "epoth: 2, iter_num: 5400, loss: 0.0848, 90.00%\n",
      "epoth: 2, iter_num: 5500, loss: 0.0217, 91.67%\n",
      "epoth: 2, iter_num: 5600, loss: 0.0618, 93.33%\n",
      "epoth: 2, iter_num: 5700, loss: 0.0091, 95.00%\n",
      "epoth: 2, iter_num: 5800, loss: 0.0103, 96.67%\n",
      "epoth: 2, iter_num: 5900, loss: 0.3939, 98.33%\n",
      "epoth: 2, iter_num: 6000, loss: 0.2979, 100.00%\n",
      "Epoch: 2, Average training loss: 0.1456\n",
      "Accuracy: 0.9325\n",
      "Average testing loss: 0.2329\n",
      "-------------------------------\n",
      "------------Epoch: 3 ----------------\n",
      "epoth: 3, iter_num: 100, loss: 0.0069, 1.67%\n",
      "epoth: 3, iter_num: 200, loss: 0.1599, 3.33%\n",
      "epoth: 3, iter_num: 300, loss: 0.4719, 5.00%\n",
      "epoth: 3, iter_num: 400, loss: 0.1976, 6.67%\n",
      "epoth: 3, iter_num: 500, loss: 0.0281, 8.33%\n",
      "epoth: 3, iter_num: 600, loss: 0.0320, 10.00%\n",
      "epoth: 3, iter_num: 700, loss: 0.0094, 11.67%\n",
      "epoth: 3, iter_num: 800, loss: 0.0129, 13.33%\n",
      "epoth: 3, iter_num: 900, loss: 0.1735, 15.00%\n",
      "epoth: 3, iter_num: 1000, loss: 0.1311, 16.67%\n",
      "epoth: 3, iter_num: 1100, loss: 0.1293, 18.33%\n",
      "epoth: 3, iter_num: 1200, loss: 0.0214, 20.00%\n",
      "epoth: 3, iter_num: 1300, loss: 0.2394, 21.67%\n",
      "epoth: 3, iter_num: 1400, loss: 0.1396, 23.33%\n",
      "epoth: 3, iter_num: 1500, loss: 0.1740, 25.00%\n",
      "epoth: 3, iter_num: 1600, loss: 0.2051, 26.67%\n",
      "epoth: 3, iter_num: 1700, loss: 0.3638, 28.33%\n",
      "epoth: 3, iter_num: 1800, loss: 0.0217, 30.00%\n",
      "epoth: 3, iter_num: 1900, loss: 0.0262, 31.67%\n",
      "epoth: 3, iter_num: 2000, loss: 0.4578, 33.33%\n",
      "epoth: 3, iter_num: 2100, loss: 0.0024, 35.00%\n",
      "epoth: 3, iter_num: 2200, loss: 0.0194, 36.67%\n",
      "epoth: 3, iter_num: 2300, loss: 0.0136, 38.33%\n",
      "epoth: 3, iter_num: 2400, loss: 0.0173, 40.00%\n",
      "epoth: 3, iter_num: 2500, loss: 0.0173, 41.67%\n",
      "epoth: 3, iter_num: 2600, loss: 0.0239, 43.33%\n",
      "epoth: 3, iter_num: 2700, loss: 0.3040, 45.00%\n",
      "epoth: 3, iter_num: 2800, loss: 0.0174, 46.67%\n",
      "epoth: 3, iter_num: 2900, loss: 0.1609, 48.33%\n",
      "epoth: 3, iter_num: 3000, loss: 0.8946, 50.00%\n",
      "epoth: 3, iter_num: 3100, loss: 0.1226, 51.67%\n",
      "epoth: 3, iter_num: 3200, loss: 0.2811, 53.33%\n",
      "epoth: 3, iter_num: 3300, loss: 0.3131, 55.00%\n",
      "epoth: 3, iter_num: 3400, loss: 0.4130, 56.67%\n",
      "epoth: 3, iter_num: 3500, loss: 0.1336, 58.33%\n",
      "epoth: 3, iter_num: 3600, loss: 0.0062, 60.00%\n",
      "epoth: 3, iter_num: 3700, loss: 0.0137, 61.67%\n",
      "epoth: 3, iter_num: 3800, loss: 0.1417, 63.33%\n",
      "epoth: 3, iter_num: 3900, loss: 0.4170, 65.00%\n",
      "epoth: 3, iter_num: 4000, loss: 0.4899, 66.67%\n",
      "epoth: 3, iter_num: 4100, loss: 0.0220, 68.33%\n",
      "epoth: 3, iter_num: 4200, loss: 0.0517, 70.00%\n",
      "epoth: 3, iter_num: 4300, loss: 0.1418, 71.67%\n",
      "epoth: 3, iter_num: 4400, loss: 0.2917, 73.33%\n",
      "epoth: 3, iter_num: 4500, loss: 0.2099, 75.00%\n",
      "epoth: 3, iter_num: 4600, loss: 0.1818, 76.67%\n",
      "epoth: 3, iter_num: 4700, loss: 0.0170, 78.33%\n",
      "epoth: 3, iter_num: 4800, loss: 0.1996, 80.00%\n",
      "epoth: 3, iter_num: 4900, loss: 0.8169, 81.67%\n",
      "epoth: 3, iter_num: 5000, loss: 0.0210, 83.33%\n",
      "epoth: 3, iter_num: 5100, loss: 0.0177, 85.00%\n",
      "epoth: 3, iter_num: 5200, loss: 0.2045, 86.67%\n",
      "epoth: 3, iter_num: 5300, loss: 0.4996, 88.33%\n",
      "epoth: 3, iter_num: 5400, loss: 0.2821, 90.00%\n",
      "epoth: 3, iter_num: 5500, loss: 0.0415, 91.67%\n",
      "epoth: 3, iter_num: 5600, loss: 0.0083, 93.33%\n",
      "epoth: 3, iter_num: 5700, loss: 0.2123, 95.00%\n",
      "epoth: 3, iter_num: 5800, loss: 0.1786, 96.67%\n",
      "epoth: 3, iter_num: 5900, loss: 0.1185, 98.33%\n",
      "epoth: 3, iter_num: 6000, loss: 0.0021, 100.00%\n",
      "Epoch: 3, Average training loss: 0.1324\n",
      "Accuracy: 0.9345\n",
      "Average testing loss: 0.2093\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('/home/jiahaoli/huggingface/models/bert',num_labels=2)\n",
    "\n",
    "# 设置设备，优先使用CUDA（GPU），否则使用CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 将模型移动到指定的设备上\n",
    "model.to(device)\n",
    "\n",
    "# 定义优化器，使用AdamW，lr是学习率\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "# 定义精度计算函数\n",
    "def flat_accuracy(preds, labels):\n",
    "    # 获取预测结果的最高概率索引\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    # 展平真实标签\n",
    "    labels_flat = labels.flatten()\n",
    "    # 计算准确率\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "# -------------------------- 4. 训练和验证函数 --------------------------\n",
    "# 定义训练函数\n",
    "def train():\n",
    "    # 设置模型为训练模式\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    iter_num = 0\n",
    "    total_iter = len(train_loader)\n",
    "\n",
    "    # 遍历训练数据加载器\n",
    "    for batch in train_loader:\n",
    "        # 清除上一轮的梯度\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # 将批次数据移动到指定设备\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # 执行前向传播，得到损失和logits\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels) # 自动计算损失\n",
    "        loss = outputs[0]\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # 反向传播计算梯度\n",
    "        loss.backward()\n",
    "        # 梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 更新模型参数\n",
    "        optim.step()\n",
    "\n",
    "        iter_num += 1\n",
    "        # 每100步打印一次训练进度\n",
    "        if (iter_num % 100 == 0):\n",
    "            print(\"epoth: %d, iter_num: %d, loss: %.4f, %.2f%%\" % (\n",
    "                epoch, iter_num, loss.item(), iter_num / total_iter * 100))\n",
    "\n",
    "    # 打印平均训练损失\n",
    "    print(\"Epoch: %d, Average training loss: %.4f\" % (epoch, total_train_loss / len(train_loader)))\n",
    "\n",
    "\n",
    "# 定义验证函数\n",
    "def validation():\n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    # 遍历测试数据加载器\n",
    "    for batch in test_dataloader:\n",
    "        # 在验证阶段，不计算梯度\n",
    "        with torch.no_grad():\n",
    "            # 将批次数据移动到指定设备\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 执行前向传播\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        # 将logits和标签从GPU移动到CPU，并转换为numpy数组\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # 计算平均准确率\n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print(\"Accuracy: %.4f\" % (avg_val_accuracy))\n",
    "    print(\"Average testing loss: %.4f\" % (total_eval_loss / len(test_dataloader)))\n",
    "    print(\"-------------------------------\")\n",
    "\n",
    "\n",
    "# -------------------------- 5. 主训练循环 --------------------------\n",
    "# 循环训练4个epoch\n",
    "for epoch in range(4):\n",
    "    print(\"------------Epoch: %d ----------------\" % epoch)\n",
    "    # 训练模型\n",
    "    train()\n",
    "    # 验证模型\n",
    "    validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2c80ab43-86a6-41c1-993c-18c6f5c1d5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Probability: 0.0021332786418497562\n",
      "Positive Probability: 0.9996306896209717\n"
     ]
    }
   ],
   "source": [
    "# 使用微调后的模型进行预测\n",
    "def predict_sentiment(sentence):\n",
    "    inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=64, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    positive_prob = probs[0][1].item()  # 1表示正面\n",
    "    print(\"Positive Probability:\", positive_prob)\n",
    "\n",
    "# 测试一个句子\n",
    "predict_sentiment(\"我讨厌你\")\n",
    "\n",
    "# 测试一个句子\n",
    "predict_sentiment(\"我喜欢你\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526a69d-ddb8-4917-836e-770993f7d853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
